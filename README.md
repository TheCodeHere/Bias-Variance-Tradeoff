# Bias-Variance
The bias-variance tradeoff, a fundamental concept in Machine Learning, tell us that the generalization (test) error, which is the error in unseen data, can be decomposed in bias error (error from wrong model assumptions), variance (error from sensitivity to small fluctuations in training data) and irreducible error (inherent noise in the problem itself).

This little program is inspired in the article found in: https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9

The program can run in isolation to show how different realizations of the training data affect our models, how too complex models can lead to overfitting, and how too simple models can lead to underfitting.
